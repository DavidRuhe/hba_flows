{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ffe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def set_seed(seed: int, workers: bool = False):\n",
    "    seed = int(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a59919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch import nn, optim, distributions\n",
    "from tqdm import tqdm\n",
    "from pyro import distributions as dist\n",
    "from pyro.distributions import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae657c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_transform = T.ExpTransform()\n",
    "log_transform = T.ExpTransform().inv\n",
    "\n",
    "logit_transform = T.SigmoidTransform().inv\n",
    "sigmoid_transform = T.SigmoidTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8af4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_RNG = (0.2, 100)\n",
    "Q_RNG = (0.01, 0.99)\n",
    "Z_RNG = (0.1, 3)\n",
    "CHI_RNG = (-1, 1)\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    batches = None\n",
    "\n",
    "    def __init__(self, data, batch_size, shuffle=True):\n",
    "\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_length = len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        indices = list(reversed(range(0, self.data_length)))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(indices)\n",
    "\n",
    "        self.batches = [\n",
    "            indices[i : i + self.batch_size]\n",
    "            for i in range(0, self.data_length, self.batch_size)\n",
    "        ]\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if len(self.batches) > 0:\n",
    "            return self.data[self.batches.pop()]\n",
    "        raise StopIteration\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / self.batch_size)\n",
    "\n",
    "\n",
    "class TensorDataset:\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "        self.len = len(tensor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "        self.len = max(len(d) for d in self.datasets)\n",
    "        super().__init__()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, slice):\n",
    "            index = list(range(*index.indices(len(self))))\n",
    "        index = torch.tensor(index)\n",
    "        return torch.stack([d[index % len(d)] for d in self.datasets], dim=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "class ConcatDataLoader:\n",
    "    iterable = None\n",
    "\n",
    "    def __init__(self, *dataloaders):\n",
    "        self.dataloaders = dataloaders\n",
    "\n",
    "        i, max_size = None, 0\n",
    "        for j, loader in enumerate(self.dataloaders):\n",
    "            length = len(loader)\n",
    "            if len(loader) > max_size:\n",
    "                i, max_size = j, length\n",
    "        self.i = i\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        loaders = [\n",
    "            loader if j == self.i else cycle(loader)\n",
    "            for j, loader in enumerate(self.dataloaders)\n",
    "        ]\n",
    "        self.iterable = iter(zip(*loaders))\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.iterable)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_size\n",
    "\n",
    "\n",
    "def process_gw_data(path, normalize_fn=None):\n",
    "    events = np.load(path, allow_pickle=True)\n",
    "    datasets = []\n",
    "    m1min, m1max = float(\"inf\"), -float(\"inf\")\n",
    "    m2min, m2max = float(\"inf\"), -float(\"inf\")\n",
    "    zmin, zmax = float(\"inf\"), -float(\"inf\")\n",
    "\n",
    "    chimin, chimax = float(\"inf\"), -float(\"inf\")\n",
    "    for n, event in events.items():\n",
    "        m1 = torch.from_numpy(event[\"m1\"])\n",
    "        m1 = m1.clamp(*M_RNG)\n",
    "        m1min = min(m1min, m1.min())\n",
    "        m1max = max(m1max, m1.max())\n",
    "#         m1 = log_transform(m1)\n",
    "      \n",
    "        m2 = torch.from_numpy(event[\"m2\"])\n",
    "        m2 = m2.clamp(*M_RNG)\n",
    "        m2min = min(m2min, m2.min())\n",
    "        m2max = max(m2max, m2.max())\n",
    "#         m2 = log_transform(m2)\n",
    "\n",
    "        \n",
    "        z = torch.from_numpy(event[\"z\"])\n",
    "        z = z.clamp(*Z_RNG)\n",
    "        zmin = min(zmin, z.min())\n",
    "        zmax = max(zmax, z.max())\n",
    "\n",
    "        chi = torch.from_numpy(event[\"Xeff\"])\n",
    "        chi = chi.clamp(*CHI_RNG)\n",
    "        chimin = min(chimin, chi.min())\n",
    "        chimax = max(chimax, chi.max())\n",
    "\n",
    "        z_prior = torch.from_numpy(event[\"z_prior\"])\n",
    "        chi_prior = torch.from_numpy(event[\"Xeff_priors\"])\n",
    "\n",
    "        gw_data = torch.stack([m1, m2, chi, z, chi_prior, z_prior], dim=-1).float()\n",
    "\n",
    "        datasets.append(TensorDataset(gw_data))\n",
    "\n",
    "    dataset = ConcatDataset(*datasets)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = process_gw_data('../../datasets/sampleDict_FAR_1_in_1_yr.pickle')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061715e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dataset[torch.randint(0, len(dataset), (4096,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74018bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 4\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de7c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(DIM):\n",
    "    sns.kdeplot(samples.mean(0)[:, i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per iteration, we use 2 posterior samples. Few samples leave more noise in the data, resulting in less sharp\n",
    "# peaks in the final learned densities. More samples result in peakier (usually less physically plausible)\n",
    "# learned densities. Using only 1 sample usually leaves too much noise to capture high frequency \n",
    "# characteristics.\n",
    "dataloader = DataLoader(dataset, 2, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d910e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist = dist.Normal(torch.zeros(DIM, device=DEVICE), torch.ones(DIM, device=DEVICE))\n",
    "transform = [T.block_autoregressive(DIM, hidden_factors=(2, 2), activation='tanh') for _ in range(1)]\n",
    "composed_transform = T.ComposeTransformModule(transform).to(DEVICE)\n",
    "\n",
    "transformed_dist = dist.TransformedDistribution(base_dist, composed_transform.inv)\n",
    "optimizer = torch.optim.Adam(composed_transform.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8db088",
   "metadata": {},
   "outputs": [],
   "source": [
    "NX = 32\n",
    "grid_m1 = torch.linspace(*M_RNG, NX)\n",
    "grid_m2 = torch.linspace(*M_RNG, NX)\n",
    "grid_X = torch.linspace(*CHI_RNG, NX)\n",
    "grid_z = torch.linspace(*Z_RNG, NX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701732b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# grid_m1_ = log_transform(grid_m1)\n",
    "# grid_m2_ = log_transform(grid_m2)\n",
    "grid_m1_ = grid_m1.clone()\n",
    "grid_m2_ = grid_m2.clone()\n",
    "grid_X_ = grid_X.clone()\n",
    "grid_z_ = grid_z.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c20f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = torch.stack(torch.meshgrid(grid_m1_, grid_m2_, grid_X_, grid_z_, indexing='ij')).view(DIM, -1).T.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5235c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "steps = 6144\n",
    "train = True\n",
    "while train:\n",
    "    for theta_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        theta_batch = theta_batch.to(DEVICE).permute(1, 0, 2)\n",
    "\n",
    "        z_prior = theta_batch[:, :, 4].clone() / 1e9\n",
    "        chi_prior = theta_batch[:, :, 5].clone()\n",
    "        q = torch.tensor(1.0)\n",
    "        q = q * z_prior\n",
    "        q = q * chi_prior\n",
    "        logq = q.log()\n",
    "        \n",
    "        \n",
    "        theta_batch = theta_batch.clone()[:, :, :DIM]\n",
    "        \n",
    "        logprob = transformed_dist.log_prob(theta_batch.reshape(-1, DIM)).view(theta_batch.shape[:-1])\n",
    "\n",
    "        logprob = torch.logsumexp(logprob - logq, dim=1) - math.log(logprob.shape[1])\n",
    "\n",
    "        logprob = logprob.mean()\n",
    "        loss = -logprob\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for t in transformed_dist.transforms:\n",
    "            t.clear_cache()\n",
    "\n",
    "        if step % 128 == 0:\n",
    "            print(step, loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prob = transformed_dist.log_prob(grid).exp().view(NX, NX, NX, NX).cpu()\n",
    "\n",
    "                pm1 = prob.sum((1, 2, 3))\n",
    "                plt.plot(grid_m1_, pm1)\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "                pm2 = prob.sum((0, 2, 3))\n",
    "                plt.plot(grid_m2_, pm2)\n",
    "                plt.show()\n",
    "\n",
    "                pX = prob.sum((0, 1, 3))\n",
    "                plt.plot(grid_X_, pX)\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "                pz = prob.sum((0, 1, 2))\n",
    "                plt.plot(grid_z_, pz)\n",
    "                plt.show()\n",
    "\n",
    "        step += 1\n",
    "        if step > steps:\n",
    "            train = False\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "del prob\n",
    "prob = transformed_dist.log_prob(grid).exp().view(NX, NX, NX, NX).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9186c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inv = log_transform(grid_m1)\n",
    "inv = grid_m1\n",
    "pm1 = prob.sum((1, 2, 3)).log() # + log_transform.log_abs_det_jacobian(grid_m1, inv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24044875",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(grid_m1, pm1.detach())\n",
    "plt.ylim(-5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8777c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inv = logit_transform(grid_q)\n",
    "pm2 = prob.sum((0, 2, 3)).log() # + logit_transform.log_abs_det_jacobian(grid_q, inv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c1a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(grid_m2, pm2.detach().exp())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pX = prob.sum((0, 1, 3)).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8905a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(grid_X, pX.detach().exp())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e34bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pz = prob.sum((0, 1, 2)).log()\n",
    "plt.plot(grid_z, pz.detach().exp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e8160",
   "metadata": {},
   "source": [
    "# Sample The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a1e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = (grid_m1_.min(), grid_m1_.max()), (grid_m2_.min(), grid_m2_.max()), (grid_X_.min(), grid_X_.max()), (grid_z_.min(), grid_z_.max())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411273e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.FloatTensor(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0e982",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def rejection_sampling(fn, rng, num_samples=1024, batch_size=32768, max_log_prob=0, n_dims=1):\n",
    "    z = torch.tensor([], device=DEVICE)\n",
    "    i = 0\n",
    "    rng = torch.FloatTensor(rng)\n",
    "    max = rng[:, 1]\n",
    "    min = rng[:, 0]\n",
    "    while len(z) < num_samples:\n",
    "        eps = torch.rand((num_samples, n_dims))\n",
    "        \n",
    "        z_ = eps * (max - min) + min\n",
    "        prob = torch.rand(num_samples, device=DEVICE)\n",
    "        z_ = z_.to(DEVICE)\n",
    "        logprob = fn(z_).squeeze()\n",
    "        assert not torch.any(logprob > max_log_prob), (logprob.max(), max_log_prob)\n",
    "        prob_ = torch.exp(logprob - max_log_prob)\n",
    "        \n",
    "        accept = prob_ > prob\n",
    "        z = torch.cat([z, z_[accept, :]])\n",
    "        i += 1\n",
    "        print(len(z))\n",
    "    return z[:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c88916",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = rejection_sampling(transformed_dist.log_prob, rng, n_dims=4).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(samples[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(samples[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e39c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(samples[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c8946",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(samples[:, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78377626",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[:, 1] = samples[:, 1] / samples[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f962f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples[samples[:, 1] <= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62462ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "fig = corner.corner(samples.numpy(), labels=[r\"$m_1\\, [M_{\\odot}]$\", r\"$q$\", r\"$\\mathcal{X}_{eff}$\", r\"$z$\"], label_kwargs=dict(fontsize=22))\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# fig.subplots_adjust(right=1.5,top=1.5)\n",
    "plt.savefig('corner.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a244d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
